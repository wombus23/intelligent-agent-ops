# Intelligent Agent Ops (IAOps)

**A comprehensive LLMOps platform for multi-agent orchestration, prompt versioning, and intelligent monitoring with cost optimization.**

IAOps is an end-to-end framework designed for production LLM applications that require multi-agent collaboration, sophisticated prompt management, and detailed observability. Unlike traditional RAG evaluation frameworks, IAOps focuses on agent orchestration, prompt lifecycle management, and real-time cost tracking.

## ðŸŽ¯ Key Features

- ðŸ¤– **Multi-Agent Orchestration**: Coordinate multiple specialized AI agents with role-based task distribution
- ðŸ“ **Prompt Version Control**: Git-like versioning system for prompts with A/B testing capabilities
- ðŸ’° **Cost Tracking & Optimization**: Real-time token usage monitoring and cost analysis per agent/prompt
- ðŸ” **LangSmith Integration**: Advanced tracing and debugging for complex agent workflows
- ðŸ“Š **Performance Analytics**: Latency tracking, success rates, and quality metrics dashboard
- ðŸ”„ **Fallback Strategies**: Automatic retry logic with model degradation (GPT-4 â†’ GPT-3.5)
- âš¡ **Async Processing**: High-throughput agent coordination with concurrent execution
- ðŸŽ¨ **Interactive UI**: Streamlit dashboard for monitoring and control

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Request                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Agent Orchestrator (Main Controller)            â”‚
â”‚  - Task decomposition                                        â”‚
â”‚  - Agent selection & routing                                 â”‚
â”‚  - Workflow management                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Research Agent â”‚        â”‚  Analysis Agent â”‚
    â”‚  (Data Gather)  â”‚        â”‚  (Processing)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                          â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Synthesis Agentâ”‚
              â”‚  (Final Output) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Monitoring Layer                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  LangSmith   â”‚  â”‚ Cost Tracker  â”‚  â”‚ Prompt Versioner â”‚ â”‚
â”‚  â”‚   Tracing    â”‚  â”‚   Analytics   â”‚  â”‚   Management     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Quick Start

### Prerequisites

- Python 3.9+
- OpenAI API key
- LangSmith API key (optional but recommended)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/intelligent-agent-ops.git
cd intelligent-agent-ops

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Environment Setup

```bash
# Create .env file
cat > .env << EOF
OPENAI_API_KEY=your_openai_api_key
LANGCHAIN_API_KEY=your_langsmith_api_key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=intelligent-agent-ops
ENABLE_COST_TRACKING=true
ENABLE_PROMPT_VERSIONING=true
MAX_RETRIES=3
DEFAULT_MODEL=gpt-4o-mini
FALLBACK_MODEL=gpt-3.5-turbo
EOF
```

## ðŸ“ Project Structure

```
intelligent-agent-ops/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py          # Base agent class
â”‚   â”œâ”€â”€ research_agent.py      # Research & data gathering
â”‚   â”œâ”€â”€ analysis_agent.py      # Data analysis & processing
â”‚   â”œâ”€â”€ synthesis_agent.py     # Final output generation
â”‚   â””â”€â”€ orchestrator.py        # Main orchestration logic
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ cost_tracker.py        # Token & cost monitoring
â”‚   â”œâ”€â”€ prompt_manager.py      # Prompt versioning system
â”‚   â””â”€â”€ observability.py       # LangSmith integration
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ retry_logic.py         # Fallback strategies
â”‚   â”œâ”€â”€ metrics.py             # Performance metrics
â”‚   â””â”€â”€ validators.py          # Output validation
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ dashboard.py           # Streamlit dashboard
â”‚   â””â”€â”€ components/            # UI components
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”œâ”€â”€ test_orchestrator.py
â”‚   â”œâ”€â”€ test_cost_tracker.py
â”‚   â””â”€â”€ test_prompt_manager.py
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â”œâ”€â”€ multi_agent_workflow.py
â”‚   â””â”€â”€ prompt_versioning_demo.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ versions/              # Versioned prompt storage
â”œâ”€â”€ data/
â”‚   â””â”€â”€ analytics/             # Analytics data storage
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ðŸ’¡ Usage Examples

### Basic Multi-Agent Workflow

```python
from agents.orchestrator import AgentOrchestrator
from core.config import IAOpsConfig

# Initialize configuration
config = IAOpsConfig.from_env()

# Create orchestrator
orchestrator = AgentOrchestrator(config)

# Execute multi-agent task
result = await orchestrator.execute_task(
    task="Analyze the sentiment of tech company earnings calls from Q4 2024",
    agents=["research", "analysis", "synthesis"],
    context={"industry": "technology", "quarter": "Q4 2024"}
)

print(f"Final Result: {result.output}")
print(f"Total Cost: ${result.total_cost:.4f}")
print(f"Execution Time: {result.execution_time:.2f}s")
```

### Prompt Version Management

```python
from core.prompt_manager import PromptVersionManager

# Initialize prompt manager
pm = PromptVersionManager()

# Create new prompt version
pm.create_version(
    prompt_name="research_agent_system",
    content="You are a research assistant specializing in...",
    version="v1.2.0",
    description="Improved specificity in research instructions",
    tags=["production", "research"]
)

# A/B test prompts
results = await pm.ab_test(
    prompt_name="research_agent_system",
    versions=["v1.1.0", "v1.2.0"],
    test_cases=test_dataset,
    metrics=["accuracy", "latency", "cost"]
)

# Promote best version
pm.promote_to_production(
    prompt_name="research_agent_system",
    version="v1.2.0"
)
```

### Cost Tracking & Optimization

```python
from core.cost_tracker import CostTracker

# Initialize cost tracker
tracker = CostTracker()

# Get cost breakdown by agent
cost_report = tracker.get_agent_costs(
    start_date="2024-01-01",
    end_date="2024-01-31",
    group_by="agent"
)

# Get most expensive prompts
expensive_prompts = tracker.get_top_expensive_prompts(limit=10)

# Set cost alerts
tracker.set_alert(
    threshold=100.0,  # $100
    period="daily",
    notification_channel="email"
)
```

### Monitoring with LangSmith

```python
from core.observability import ObservabilityManager

# Initialize observability
obs = ObservabilityManager()

# Get execution traces
traces = obs.get_traces(
    agent="analysis_agent",
    start_time="2024-01-01T00:00:00Z",
    filters={"status": "error"}
)

# Analyze performance metrics
metrics = obs.analyze_performance(
    agent="synthesis_agent",
    metric="latency",
    percentile=95
)
```

## ðŸŽ›ï¸ Configuration

### Agent Configuration

```python
from core.config import AgentConfig

research_config = AgentConfig(
    name="research_agent",
    model="gpt-4o-mini",
    temperature=0.3,
    max_tokens=2000,
    timeout=30,
    retry_strategy={
        "max_retries": 3,
        "fallback_model": "gpt-3.5-turbo",
        "exponential_backoff": True
    }
)
```

### Orchestration Configuration

```python
from core.config import OrchestrationConfig

orchestration_config = OrchestrationConfig(
    max_concurrent_agents=3,
    task_timeout=120,
    enable_parallel_execution=True,
    result_aggregation_strategy="weighted_voting"
)
```

## ðŸ“Š Dashboard

Launch the interactive Streamlit dashboard:

```bash
streamlit run ui/dashboard.py
```

Features:
- Real-time agent activity monitoring
- Cost analytics and trends
- Prompt version comparison
- Execution trace visualization
- Performance metrics
- Alert management

## ðŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=. --cov-report=html

# Run specific test suite
pytest tests/test_orchestrator.py -v

# Run integration tests
pytest tests/integration/ -v
```

## ðŸ³ Docker Deployment

```bash
# Build image
docker-compose build

# Run services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

## ðŸ“ˆ Metrics & Analytics

### Agent Performance Metrics

- **Success Rate**: Percentage of successful task completions
- **Average Latency**: Mean execution time per agent
- **Token Efficiency**: Tokens per task / output quality ratio
- **Error Rate**: Frequency of failures and retries

### Cost Metrics

- **Cost per Agent**: Total spend broken down by agent type
- **Cost per Task**: Average cost for different task categories
- **Token Usage**: Input/output token distribution
- **Model Distribution**: Usage patterns across different models

### Prompt Metrics

- **Version Performance**: Quality metrics per prompt version
- **A/B Test Results**: Statistical significance of version differences
- **Adoption Rate**: Speed of production rollout
- **Rollback Frequency**: Stability indicator

## ðŸ”§ Advanced Features

### Custom Agent Creation

```python
from agents.base_agent import BaseAgent

class CustomValidationAgent(BaseAgent):
    def __init__(self, config):
        super().__init__(config)
        self.agent_type = "validation"
    
    async def execute(self, task_input):
        # Custom validation logic
        validated_output = await self.validate_content(task_input)
        return validated_output
    
    async def validate_content(self, content):
        # Implementation
        pass
```

### Workflow Templates

```python
from agents.orchestrator import WorkflowTemplate

# Define custom workflow
custom_workflow = WorkflowTemplate(
    name="content_generation_workflow",
    stages=[
        {"agent": "research", "parallel": False},
        {"agent": "analysis", "parallel": False},
        {"agent": "synthesis", "parallel": False},
        {"agent": "validation", "parallel": False}
    ],
    error_handling="continue",  # or "stop"
    result_caching=True
)

# Execute workflow
result = await orchestrator.execute_workflow(
    workflow=custom_workflow,
    input_data=task_data
)
```

## ðŸ” Security & Best Practices

- Store API keys in environment variables
- Use role-based access control for prompt management
- Implement rate limiting for cost control
- Enable audit logging for compliance
- Validate all agent outputs before use
- Implement content filtering for safety

## ðŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ðŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- [LangChain](https://github.com/langchain-ai/langchain) - LLM framework
- [LangSmith](https://smith.langchain.com/) - Observability platform
- [OpenAI](https://openai.com/) - LLM provider
- [Streamlit](https://streamlit.io/) - Dashboard framework



## ðŸ—ºï¸ Roadmap

- [ ] Support for additional LLM providers (Anthropic, Cohere, Mistral)
- [ ] Advanced agent memory systems
- [ ] Graph-based workflow orchestration
- [ ] Real-time streaming support
- [ ] Custom evaluation metrics framework
- [ ] Enterprise SSO integration
- [ ] Multi-tenancy support
- [ ] Vector store integration for context management

